{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NMF.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"cD56XSlwmfYh"},"source":["# Prep"]},{"cell_type":"markdown","metadata":{"id":"GZTh_8KUlT9b"},"source":["## Import Packages and Mount Drive "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rtfjwrLx60jt","executionInfo":{"status":"ok","timestamp":1626797377234,"user_tz":420,"elapsed":15769,"user":{"displayName":"Tesla Zhu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgVPHi7IxKkHrrk42LFP8DocXDP-GNZvpOORfuQ=s64","userId":"14366854534698990609"}},"outputId":"cc3cb97c-23ce-438c-de26-6554f98bd000"},"source":["from google.colab import drive \n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SfcBaeYalYzn","executionInfo":{"status":"ok","timestamp":1626797381234,"user_tz":420,"elapsed":4004,"user":{"displayName":"Tesla Zhu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgVPHi7IxKkHrrk42LFP8DocXDP-GNZvpOORfuQ=s64","userId":"14366854534698990609"}},"outputId":"c66ab46a-2591-48c9-ba86-1d5259d64d38"},"source":["import glob, json, zipfile, os\n","\n","import pandas as pd\n","# from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","import os\n","import glob\n","import json\n","import pandas as pd\n","import re\n","import nltk\n","import copy as cp\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('stopwords')\n","nltk.download('wordnet') \n","nltk.download('vader_lexicon')\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import stopwords, wordnet\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","from tqdm.notebook import tqdm as tq\n","tq.pandas()\n","import gensim.corpora as corpora\n","from gensim.models import CoherenceModel, LdaMulticore\n","from gensim.utils import simple_preprocess \n","import gensim.corpora as corpora\n","from google.colab import files\n","analyzer = SentimentIntensityAnalyzer()\n","\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n","  warnings.warn(\"The twython library has not been installed. \"\n"],"name":"stderr"},{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n","[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n","  from pandas import Panel\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"16bPH5RPgBDy"},"source":["## Initalize Paths"]},{"cell_type":"code","metadata":{"id":"wuO8DiCExPmj"},"source":["CHECKPOINT_PATH = '/content/drive/MyDrive/UCLA REU 2021 KG /Topic Modeling/NMF/months/'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y7RdjXGnAblW"},"source":["## Initialize Checkpoint I/O"]},{"cell_type":"code","metadata":{"id":"YMk7l8jnzLGS"},"source":["def write_data(filename, df, path):\n","  \n","  with open(path + '/'+ filename, 'a') as f: \n","      result = df.to_json(orient = 'records', double_precision = 15)\n","      parsed = json.loads(result)\n","      f.write(json.dumps(parsed))\n","  f.close()\n","\n","def load_checkpoint(filename, path):\n","  \n","  with open(path + '/'+ filename, 'r') as f: \n","    read = pd.read_json(f, \n","                        orient='records', keep_default_dates=False, precise_float=True) \n","  return pd.DataFrame(read)\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l31bW7kylQvy"},"source":["## Load Data"]},{"cell_type":"code","metadata":{"id":"XQPeoQoSCbrY"},"source":["# cleaned_df = load_checkpoint('document_data_mar16-apr07.json','/content/drive/MyDrive/UCLA REU 2021 KG /results')\n","cleaned_df = load_checkpoint('processed_tweets_20mar11-21june16.json','/content/drive/MyDrive/UCLA REU 2021 KG /results/2020Mar11-2021June16')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ysx4rVFfrxUL"},"source":["with open('/content/drive/MyDrive/UCLA REU 2021 KG /results/2020Mar11-2021June16/nmf_tweet_data_kw_senti_20mar11-21jun16_(100,20).json','r') as file:\n","  data = json.load(file)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c6SEfNm5sFbK"},"source":["with open(CHECKPOINT_PATH+'/W_(100,20).json','r') as file:\n","  W = [json.loads(line) for line in file]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1SMfhnTasSbY","executionInfo":{"status":"ok","timestamp":1626797627475,"user_tz":420,"elapsed":123,"user":{"displayName":"Tesla Zhu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgVPHi7IxKkHrrk42LFP8DocXDP-GNZvpOORfuQ=s64","userId":"14366854534698990609"}},"outputId":"4b62e88f-e603-4449-f9f3-e71ff77ba278"},"source":["print(W[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[0.004532880321274728, 0.00020720658069324152, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31634372711467745, 0.0, 0.0, 0.022516416127119333, 0.0, 0.0, 0.0, 0.008660488441208578, 0.010964927627729248, 0.0, 0.0, 0.004973057091413448, 0.0, 0.0039022181979092726, 0.49201421858406574, 0.0004348708772802355, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00941448141869875, 0.0, 0.0, 0.0010122221626522158, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05293271412643608, 0.0, 0.0, 0.0007192764658308471, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005277580591659191, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0059205486155976365, 0.0, 0.0, 0.0, 0.0, 0.05453490097674192, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0008282694455216886, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00480999523349028, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dPoyKS2KscEs"},"source":["topic_threshold = 0.01\n","topic_minimum = 2 \n","def cleanTopics(topics):\n","  nTopics = normalizeWeight(list(topics))\n","  return [[i,nTopics[i]] for i in range(len(nTopics)) \n","    if ((i<topic_minimum) or (nTopics[i]>=topic_threshold))]\n","\n","processedW = [cleanTopics(w) for w in W]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qSzAVebes4Qz"},"source":["for i in range(len(data)):\n","  data[i]['nmf_topics'] = [v[0] for v in processedW[i]]\n","  data[i]['nmf_topic_distribution'] = processedW[i]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0w26G1XbtdIr","executionInfo":{"status":"ok","timestamp":1626797902355,"user_tz":420,"elapsed":162,"user":{"displayName":"Tesla Zhu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgVPHi7IxKkHrrk42LFP8DocXDP-GNZvpOORfuQ=s64","userId":"14366854534698990609"}},"outputId":"f8c6d650-7150-43d5-8710-9dce78ecb821"},"source":["print(data[0]['nmf_topic_distribution'])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[0, 0.004532880321274729], [1, 0.00020720658069324155], [7, 0.3163437271146775], [10, 0.022516416127119337], [15, 0.01096492762772925], [21, 0.4920142185840658], [53, 0.05293271412643609], [76, 0.05453490097674193]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"n5ZmlKeztbIR"},"source":["with open(CHECKPOINT_PATH+'/NMF_data.json','w') as file:\n","  json.dump(data, file)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qqZ-guN2yaTq"},"source":["def vaderScore(text):\n","  return analyzer.polarity_scores(text)['compound']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h6xn84U4zOZq","executionInfo":{"status":"ok","timestamp":1626799760461,"user_tz":420,"elapsed":32467,"user":{"displayName":"Tesla Zhu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgVPHi7IxKkHrrk42LFP8DocXDP-GNZvpOORfuQ=s64","userId":"14366854534698990609"}},"outputId":"a7b0aed0-03e6-4ce4-e871-b7afce3baf97"},"source":["for tweet in data:\n","  tweet['sentiment_score'] = vaderScore(tweet['text'])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"so9N9beh0-ad","executionInfo":{"status":"ok","timestamp":1626799864234,"user_tz":420,"elapsed":133,"user":{"displayName":"Tesla Zhu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgVPHi7IxKkHrrk42LFP8DocXDP-GNZvpOORfuQ=s64","userId":"14366854534698990609"}},"outputId":"34756eba-4334-4b12-b82e-e36c9e089153"},"source":["print(data[5]['sentiment_score'])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.5267\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ExyfZSDr36D6","executionInfo":{"status":"ok","timestamp":1626802009746,"user_tz":420,"elapsed":124,"user":{"displayName":"Tesla Zhu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgVPHi7IxKkHrrk42LFP8DocXDP-GNZvpOORfuQ=s64","userId":"14366854534698990609"}},"outputId":"4ff1eb29-52b5-4492-94ec-033d01f5a119"},"source":["import calendar\n","print(data[0]['timestamp'])\n","print(data[0]['timestamp'][4:8])\n","print(data[0]['timestamp'][-4:])\n","print(data[0]['timestamp'][4:8]+data[0]['timestamp'][-4:])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Wed Mar 11 07:01:18 +0000 2020\n","Mar \n","2020\n","Mar 2020\n","{'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4, 'May': 5, 'Jun': 6, 'Jul': 7, 'Aug': 8, 'Sep': 9, 'Oct': 10, 'Nov': 11, 'Dec': 12}\n","Oct 2020\n","202106\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"d-i6S8Bq9hFy"},"source":["month_dict = {month: index for index, month in enumerate(calendar.month_abbr) if month}\n","month_list = list(month_dict)\n","\n","# timestamp to text: 202003->Mar 2020\n","def timeToText(time:int):\n","  year = time//100\n","  month = time%100\n","  return month_list[month-1]+' '+str(year)\n","\n","def textToTime(text:str):\n","  year = text[-4:]\n","  month = text[:-5]\n","  return 100*int(year)+month_dict[month]\n","\n","for tweet in data:\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O8Ux-Ue9lyVv"},"source":["## Keyword Extraction using TFIDF\n"]},{"cell_type":"markdown","metadata":{"id":"Q4SVVoVS4f-o"},"source":["Extracts top $n$ keywords for each tweet. "]},{"cell_type":"code","metadata":{"id":"rj1aNs1mlH2s"},"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.cluster import KMeans\n","from operator import itemgetter\n","\n","docs = cp.copy(cleaned_df)['cleaned_text'].to_list()\n","all_text = []\n","for tweet in docs:\n","  for word in tweet:\n","    all_text.append(word)\n","\n","id2word = corpora.Dictionary(docs)\n","# Ceate Corpus\n","texts = docs\n","\n","corpus = [id2word.doc2bow(text) for text in texts]\n","\n","vectorizer = TfidfVectorizer()\n","\n","for i in range(len(texts)):\n","  texts[i] = ' '.join(texts[i])\n","\n","tfidf = vectorizer.fit_transform(texts)\n","\n","feature_names = vectorizer.get_feature_names()\n","\n","tc = tfidf.tocoo()\n","\n","doc_dict = {}\n","for w,d,s in zip(tc.col, tc.row, tc.data):\n","  word_id = w\n","  word = feature_names[word_id]\n","  doc_num = d\n","  score = s \n","  if doc_num in doc_dict.keys():\n","    doc_dict[doc_num][word] = score\n","  else:\n","    doc_dict[doc_num] = {word: score} \n","\n","tweets_df = cp.copy(cleaned_df)\n","tweets_df['keywords'] = tweets_df.apply(lambda x: [], axis=1)\n","\n","\n","N = 3\n","all_keywords = []\n","for doc in doc_dict.keys():\n","  res = dict(sorted(doc_dict[doc].items(), key = itemgetter(1), reverse = True)[:N])\n","\n","  keywords = list(res.keys())\n","\n","  all_keywords.append(keywords)\n","  tweets_df.at[doc, 'keywords']= keywords\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PlAo-70ZDMyV","executionInfo":{"status":"ok","timestamp":1626717599093,"user_tz":420,"elapsed":32934,"user":{"displayName":"Tesla Zhu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgVPHi7IxKkHrrk42LFP8DocXDP-GNZvpOORfuQ=s64","userId":"14366854534698990609"}},"outputId":"27df8e0c-4222-4180-9295-0c0f9806fd5b"},"source":["# write_data('tweets_keywords.json', tweets_df, CHECKPOINT_PATH)\n","# print(\"Successfully Saved Checkpoint #3\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Successfully Saved Checkpoint #3\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"keBV9kchlyVZ"},"source":["## Bag of Words"]},{"cell_type":"markdown","metadata":{"id":"ItY4Kau66uz5"},"source":["Creates corpus in bag-of-words format"]},{"cell_type":"code","metadata":{"id":"yhNCjxre6oZy"},"source":["# Create set of words from all tweets\n","docs = tweets_df['cleaned_text'].to_list()\n","all_text = []\n","for tweet in docs:\n","  for word in tweet:\n","    all_text.append(word)\n","\n","# Create Dictionary \n","id2word = corpora.Dictionary(docs)\n","\n","# Ceate Corpus\n","texts = docs\n","\n","# Term Document Frequency \n","corpus = [id2word.doc2bow(text) for text in texts]\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UfeY3hSKo03h"},"source":["## Helper Functions"]},{"cell_type":"code","metadata":{"id":"gKHZlFIqo565"},"source":["import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# Use sklearn model\n","def plot_top_words(model, feature_names, n_top_words, title):\n","    fig, axes = plt.subplots(n_components//5, 5, figsize=(30, int(1.5*n_components)), sharex=True) # change num of topics?\n","    axes = axes.flatten()\n","    for topic_idx, topic in enumerate(model.components_):\n","        top_features_ind = topic.argsort()[:-n_top_words - 1:-1]\n","        top_features = [feature_names[i] for i in top_features_ind]\n","        weights = topic[top_features_ind]\n","\n","        ax = axes[topic_idx]\n","        ax.barh(top_features, weights, height=0.7)\n","        ax.set_title(f'Topic {topic_idx}',\n","                     fontdict={'fontsize': 30})\n","        ax.invert_yaxis()\n","        ax.tick_params(axis='both', which='major', labelsize=20)\n","        for i in 'top right left'.split():\n","            ax.spines[i].set_visible(False)\n","        fig.suptitle(title, fontsize=40)\n","\n","    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n","    plt.show()\n","\n","# Use lists of dict, each dict contains keyword-weight pairs\n","def plot_top_words_dict(topics, title):\n","  fig, axes = plt.subplots(n_components//5, 5, figsize=(30, int(1.5*n_components)), sharex=True) # change num of topics?\n","  axes = axes.flatten()\n","  indx = 0 # topic index\n","  for topic in topics:\n","    keywords = list(topic)\n","    weights = [topic[word] for word in keywords]\n","\n","    ax = axes[indx]\n","    ax.barh(keywords, weights, height=0.7)\n","    ax.set_title(f'Topic {indx}',\n","                  fontdict={'fontsize': 30})\n","    ax.invert_yaxis()\n","    ax.tick_params(axis='both', which='major', labelsize=20)\n","    for i in 'top right left'.split():\n","        ax.spines[i].set_visible(False)\n","    fig.suptitle(title, fontsize=40)\n","    indx+=1\n","\n","\n","def normalizeWeight(arr): # so that they sum to 1\n","  return [x/sum(arr) for x in arr] if sum(arr) else []"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MxyFqLnqwJcL"},"source":["# NMF\n","\n"]},{"cell_type":"code","metadata":{"id":"VxTVrh5inGTR"},"source":["from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n","from sklearn.decomposition import NMF\n","\n","# Change these parameters as necessary\n","# n_samples = 2000\n","# n_features = 1000\n","n_components = 100\n","n_top_words = 20\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O9VoOTe_n3pS"},"source":["# Frobenius norm\n","nmf = NMF(n_components=n_components, random_state=1,\n","          alpha=.1, l1_ratio=.5) #.fit(tfidf)\n","\n","W = nmf.fit_transform(tfidf)\n","# Time complexity:\n","# Sublinear in timeframe (maybe linear to amount of text?)\n","#\n","# (n_components,n_top_words) = (20, 20):\n","#   101s for ~66 weeks; [12s for 3 weeks;] \n","# (30,20): \n","#   642s\n","# (100,20):\n","#   3509s\n","# around O(n_compnents^1.5~1.7)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uJOQEz1x8drB"},"source":["topic_threshold = 0.01\n","topic_minimum = 2 \n","def cleanTopics(topics):\n","  nTopics = normalizeWeight(list(topics))\n","  return [[i,nTopics[i]] for i in range(len(nTopics)) \n","    if ((i<topic_minimum) or (nTopics[i]>=topic_threshold))]\n","\n","processedW = [cleanTopics(w) for w in W]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sgcXycYr_clx"},"source":["tweets_df['nmf_topics'] = [[v[0] for v in w] for w in processedW]\n","tweets_df['nmf_topic_distribution'] = processedW"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MJa1R1CrLQJT","executionInfo":{"status":"ok","timestamp":1626734649337,"user_tz":420,"elapsed":143,"user":{"displayName":"Tesla Zhu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgVPHi7IxKkHrrk42LFP8DocXDP-GNZvpOORfuQ=s64","userId":"14366854534698990609"}},"outputId":"6d20b34c-5e9c-4e03-fd46-a5621d5b37c4"},"source":["print(len(W))\n","print(len(tweets_df))\n","for tweet in tweets_df:\n","  print(tweet)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["613949\n","613949\n","tweet-id\n","user-id\n","user-screen_name\n","verified\n","text\n","cleaned_text\n","hashtags\n","mentions\n","in-reply-to\n","quoted\n","timestamp\n","followers-count\n","replies_count\n","quote_count\n","replies\n","quotes\n","keywords\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tbdL4c0pIKAY"},"source":["with open(CHECKPOINT_PATH+'/W_('+str(n_components)+','+str(n_top_words)+').json', 'w') as f:\n","  for w in W:\n","    f.write(str(normalizeWeight(w)))\n","    f.write('\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WiRWKZl6Iz1T"},"source":["def normalizeWeight(arr): # so that they sum to 1\n","  return [x/sum(arr) for x in arr] if sum(arr) else []\n","\n","W' = [normalizeWeight(w) for w in W]\n","\n","with open(CHECKPOINT_PATH+'W\\'_('+str(n_components)+','+str(n_top_words)+').json', 'w') as f:\n","  f.write('\\n'.join([str(w) for w in W']))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9KlwwI8nLpEB"},"source":["# generalized Kullback-Leibler divergence\n","# nmf = NMF(n_components=n_components, random_state=1,\n","#           beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n","#           l1_ratio=.5).fit(tfidf) \n","\n","# 15s\n","\n","# FN looks better for now"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":213},"id":"_baLodTspAWR","executionInfo":{"status":"error","timestamp":1626710854685,"user_tz":420,"elapsed":122,"user":{"displayName":"Tesla Zhu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgVPHi7IxKkHrrk42LFP8DocXDP-GNZvpOORfuQ=s64","userId":"14366854534698990609"}},"outputId":"50f7c21d-410a-4870-f4bd-5347ae18998b"},"source":["tfidf_feature_names = vectorizer.get_feature_names()\n","plot_top_words(nmf, tfidf_feature_names, n_top_words,\n","               'Topics in NMF model (Frobenius norm)')"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-465bb0b0d65f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtfidf_feature_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m plot_top_words(nmf, tfidf_feature_names, n_top_words,\n\u001b[1;32m      3\u001b[0m                'Topics in NMF model (Frobenius norm)')\n","\u001b[0;31mNameError\u001b[0m: name 'vectorizer' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"LoAnJPo4nGal"},"source":["## Store in DataFrame\n"]},{"cell_type":"code","metadata":{"id":"yDo1Xj7Pq3yx"},"source":["def normalizeWeight(arr): # so that they sum to 1\n","  return [x/sum(arr) for x in arr] if sum(arr) else []\n","\n","results = []\n","i=0\n","for topic_idx, topic in enumerate(nmf.components_):\n","  top_features_ind = topic.argsort()[:-n_top_words - 1:-1]\n","  top_features = [feature_names[j] for j in top_features_ind]\n","  weights = normalizeWeight(topic[top_features_ind])\n","  results.append({'topic_id':i,'keywords':top_features,'weightings':weights,\n","                  'keywords_weightings':[(top_features[j],weights[j]) for j in range(len(top_features))],\n","                  'keywords_dict':{top_features[j]:weights[j] for j in range(len(top_features))}})\n","  # format of keywords_weightings?\n","  i+=1\n","\n","topic_df = pd.DataFrame(results)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B3-AwPYfNk0Q"},"source":["plot_top_words_dict([dct['keywords_dict'] for dct in results][:10],'NMF topics')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DsDSGrxg16FX"},"source":["# Export Data\n"]},{"cell_type":"code","metadata":{"id":"LX2AXqExmJzS"},"source":["write_data('nmf_topic_data_20mar11-21jun16_('+str(n_components)+','+str(n_top_words)+').json', topic_df, CHECKPOINT_PATH)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4VHmnHNMEbJ5"},"source":["write_data('nmf_tweet_data_20mar11-21jun16_('+str(n_components)+','+str(n_top_words)+').json', cleaned_df, CHECKPOINT_PATH)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eb9e0E0c-ch3"},"source":["# Combined block"]},{"cell_type":"code","metadata":{"id":"-4h7b3Eo-gYJ"},"source":["n_components = 400\n","\n","# Calculation\n","nmf = NMF(n_components=n_components, random_state=1,\n","          alpha=.1, l1_ratio=.5).fit(tfidf)\n","\n","tfidf_feature_names = vectorizer.get_feature_names()\n","\n","# Convert result\n","results = []\n","i=0\n","for topic_idx, topic in enumerate(nmf.components_):\n","  top_features_ind = topic.argsort()[:-n_top_words - 1:-1]\n","  top_features = [feature_names[j] for j in top_features_ind]\n","  weights = normalizeWeight(topic[top_features_ind])\n","  results.append({'topic_id':i,'keywords':top_features,'weightings':weights,\n","                  'keywords_weightings':[(top_features[j],weights[j]) for j in range(len(top_features))],\n","                  'keywords_dict':{top_features[j]:weights[j] for j in range(len(top_features))}})\n","  # format of keywords_weightings?\n","  i+=1\n","\n","topic_df = pd.DataFrame(results)\n","\n","# Export Data\n","write_data('nmf_topic_data_20mar11-21jun16_('+str(n_components)+','+str(n_top_words)+').json', topic_df, CHECKPOINT_PATH)\n","\n","# Plot \n","plot_top_words(nmf, tfidf_feature_names, n_top_words,\n","               'Topics in NMF model (Frobenius norm)')\n","\n"],"execution_count":null,"outputs":[]}]}